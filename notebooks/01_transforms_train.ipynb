{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸŽ¨ Notebook 01: Train Transforms â€” Data Augmentation Pipeline\n",
        "\n",
        "**Purpose:** Build a robust augmentation pipeline for training data to improve model generalization.\n",
        "\n",
        "**What you'll learn:** How to compose transforms in the correct order, why augmentation matters, and how normalization stabilizes training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŽ¯ Concept Primer: Why Data Augmentation?\n",
        "\n",
        "### The Problem: Limited Data\n",
        "- Deep learning models need **thousands** of examples to generalize well\n",
        "- Medical imaging datasets are expensive to label (expert pathologists needed)\n",
        "- Small datasets â†’ **overfitting** (model memorizes training examples)\n",
        "\n",
        "### The Solution: Data Augmentation\n",
        "- Create **synthetic variety** by applying realistic transformations\n",
        "- Horizontal flips, rotations, color jitter â†’ model learns invariant features\n",
        "- **Only applied to training data** (val/test need consistency)\n",
        "\n",
        "### Transform Order Matters!\n",
        "```\n",
        "âœ… CORRECT:\n",
        "Resize â†’ Augmentation (Flip, Rotate, ColorJitter) â†’ ToTensor â†’ Normalize\n",
        "\n",
        "âŒ WRONG:\n",
        "ToTensor â†’ ColorJitter  (ColorJitter expects PIL images, not tensors!)\n",
        "Normalize â†’ Resize  (Normalize expects [0,1] tensor values)\n",
        "```\n",
        "\n",
        "### Normalization Deep Dive\n",
        "- `ToTensor()` converts PIL image [0,255] â†’ tensor [0,1]\n",
        "- `Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])` converts [0,1] â†’ [-1,1]\n",
        "- **Formula:** `output = (input - mean) / std`\n",
        "- **Why?** Centered data â†’ stable gradients â†’ faster convergence\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“š Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "\n",
        "1. âœ… Build `train_transform` with `transforms.Compose()`\n",
        "2. âœ… Apply augmentations in the correct order\n",
        "3. âœ… Understand which augmentations are realistic for histopathology\n",
        "4. âœ… Normalize images to stabilize training\n",
        "5. âœ… Verify transform output shape: `[3, 96, 96]`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âœ… Acceptance Criteria\n",
        "\n",
        "Your transform pipeline is correct when:\n",
        "\n",
        "- [ ] `train_transform` is a `transforms.Compose` object\n",
        "- [ ] Transforms are in order: Resize â†’ RandomHorizontalFlip â†’ RandomRotation â†’ ColorJitter â†’ ToTensor â†’ Normalize\n",
        "- [ ] Applying transform to a sample image produces a tensor of shape `[3, 96, 96]`\n",
        "- [ ] Tensor values are in range `[-1, 1]` (after normalization)\n",
        "- [ ] You can explain why ColorJitter comes **before** ToTensor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ’» TODO 1: Import Required Libraries\n",
        "\n",
        "**What you need:**\n",
        "- `torchvision.transforms` for transform classes\n",
        "- `PIL.Image` to test loading a sample image\n",
        "\n",
        "**Expected behavior:** Imports run without errors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 1: Import transforms and Image\n",
        "# Hint: from torchvision import transforms\n",
        "# Hint: from PIL import Image\n",
        "\n",
        "# YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ’» TODO 2: Build the Training Transform Pipeline\n",
        "\n",
        "**What you need to compose (in this order):**\n",
        "\n",
        "1. **`transforms.Resize((96, 96))`** â€” Ensure all images are 96Ã—96\n",
        "2. **`transforms.RandomHorizontalFlip(p=0.5)`** â€” Flip left-right 50% of the time\n",
        "3. **`transforms.RandomRotation(degrees=15)`** â€” Rotate Â±15Â° randomly\n",
        "4. **`transforms.ColorJitter(brightness=0.2, contrast=0.2)`** â€” Vary brightness/contrast\n",
        "5. **`transforms.ToTensor()`** â€” Convert PIL image â†’ tensor [0,1]\n",
        "6. **`transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])`** â€” Normalize to [-1,1]\n",
        "\n",
        "**Expected output:** A `transforms.Compose` object stored in `train_transform`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 2: Create train_transform using transforms.Compose()\n",
        "# Hint: train_transform = transforms.Compose([...])\n",
        "# Hint: List the 6 transforms above in the correct order\n",
        "\n",
        "# YOUR CODE HERE\n",
        "train_transform = None  # Replace this line\n",
        "\n",
        "print(\"âœ… train_transform created:\")\n",
        "print(train_transform)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ’» TODO 3: Test the Transform on a Sample Image\n",
        "\n",
        "**What you need to do:**\n",
        "1. Load a sample image from `../data/pcam_images/` (pick any `.png` file)\n",
        "2. Apply `train_transform` to the image\n",
        "3. Print the shape of the resulting tensor\n",
        "\n",
        "**Expected output:**\n",
        "```\n",
        "Original image: PIL Image object\n",
        "Transformed tensor shape: torch.Size([3, 96, 96])\n",
        "Tensor value range: approximately [-1, 1]\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 3: Test transform on a sample image\n",
        "# Hint: sample_img = Image.open('../data/pcam_images/SOME_FILE.png')\n",
        "# Hint: transformed = train_transform(sample_img)\n",
        "# Hint: print(transformed.shape)\n",
        "\n",
        "import os\n",
        "\n",
        "# Get a sample image path\n",
        "sample_images = os.listdir('../data/pcam_images/')\n",
        "sample_path = os.path.join('../data/pcam_images/', sample_images[0])\n",
        "\n",
        "# YOUR CODE HERE\n",
        "# Load the image\n",
        "# Apply train_transform\n",
        "# Print the shape and value range\n",
        "\n",
        "print(f\"âœ… Sample image path: {sample_path}\")\n",
        "# YOUR PRINTS HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ¤” Reflection Prompts\n",
        "\n",
        "### Question 1: Realistic Augmentations for Histopathology\n",
        "Which of the following augmentations are **realistic** for H&E-stained pathology slides, and which might **distort clinically relevant information**?\n",
        "\n",
        "| Augmentation | Realistic? | Reasoning |\n",
        "|--------------|------------|-----------|\n",
        "| RandomHorizontalFlip | âœ… / âŒ | ? |\n",
        "| RandomVerticalFlip | âœ… / âŒ | ? |\n",
        "| RandomRotation(Â±15Â°) | âœ… / âŒ | ? |\n",
        "| RandomRotation(Â±180Â°) | âœ… / âŒ | ? |\n",
        "| ColorJitter(brightness=0.2) | âœ… / âŒ | ? |\n",
        "| ColorJitter(hue=0.5) | âœ… / âŒ | ? |\n",
        "| RandomGrayscale | âœ… / âŒ | ? |\n",
        "\n",
        "**Your analysis:**\n",
        "\n",
        "---\n",
        "\n",
        "### Question 2: Why Not Augment Validation/Test Data?\n",
        "Explain in your own words:\n",
        "- Why do we apply augmentation to training data?\n",
        "- Why would augmentation **break** validation/test evaluation?\n",
        "\n",
        "**Your explanation:**\n",
        "\n",
        "---\n",
        "\n",
        "### Question 3: Normalization Intuition\n",
        "Given:\n",
        "- `ToTensor()` converts RGB [0,255] â†’ [0,1]\n",
        "- `Normalize(mean=[0.5]*3, std=[0.5]*3)` converts [0,1] â†’ [-1,1]\n",
        "\n",
        "Calculate:\n",
        "- If a pixel value is `0.8` after `ToTensor()`, what is it after `Normalize`?\n",
        "- **Formula:** `(input - mean) / std`\n",
        "\n",
        "**Your calculation:**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# .cursorrules\n",
        "# Francisco Barbosa â€“ AI Mentor Mode\n",
        "# Purpose: enforce discovery of higher-order ML/engineering practices in every Jupyter notebook.\n",
        "\n",
        "rules:\n",
        "  # 1. Detect CNN or image projects missing transfer learning\n",
        "  - trigger: on_notebook_open\n",
        "    condition: notebook_contains(\"torch\") and not notebook_contains(\"resnet\") and not notebook_contains(\"efficientnet\")\n",
        "    action: >\n",
        "      Insert_after_first_imports:\n",
        "        - markdown: |\n",
        "            ### ðŸ§© Learning Checkpoint: Transfer Learning with ResNet\n",
        "            Youâ€™re using CNNs but not leveraging pretrained backbones.\n",
        "            **Task:** integrate a pretrained ResNet (or EfficientNet) and compare its validation accuracy vs. your scratch model.\n",
        "            *Why:* Transfer learning dramatically reduces training time and improves generalization.\n",
        "        - code: |\n",
        "            # TODO: Implement transfer learning\n",
        "            from torchvision import models\n",
        "            resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "            for param in resnet.parameters():\n",
        "                param.requires_grad = False  # Freeze layers\n",
        "            # Replace final layer for your num_classes and fine-tune\n",
        "            # Compare performance with your baseline model.\n",
        "\n",
        "  # 2. Detect missing explainability\n",
        "  - trigger: on_notebook_save\n",
        "    condition: model_trained == true and not notebook_contains(\"shap\") and not notebook_contains(\"lime\")\n",
        "    action: >\n",
        "      append_to_notebook_end:\n",
        "        - markdown: |\n",
        "            ### ðŸ” Learning Checkpoint: Model Explainability\n",
        "            Youâ€™ve trained a modelâ€”now interpret it.\n",
        "            **Task:** apply SHAP or LIME to visualize which features drive predictions.\n",
        "            *Why:* Understanding model reasoning prevents overfitting and bias.\n",
        "        - code: |\n",
        "            # TODO: Add model explainability\n",
        "            import shap\n",
        "            explainer = shap.Explainer(model, X_valid)\n",
        "            shap_values = explainer(X_valid)\n",
        "            shap.summary_plot(shap_values, X_valid)\n",
        "\n",
        "  # 3. Enforce testing and CI mindset\n",
        "  - trigger: on_notebook_save\n",
        "    condition: not repo_contains(\"tests/\") and notebook_contains(\"model.fit\") or notebook_contains(\"train\")\n",
        "    action: >\n",
        "      create_file(\"tests/test_model.py\", \"\"\"\n",
        "      import pytest\n",
        "\n",
        "      def test_training_runs():\n",
        "          assert True, \"Add a small unit test for your training function\"\n",
        "      \"\"\")\n",
        "      insert_markdown_at_end: |\n",
        "          ### ðŸ§ª Learning Checkpoint: Testing & Automation\n",
        "          **Task:** create minimal Pytest tests for data loading and model training.\n",
        "          Later, set up a GitHub Action to run `pytest` on every push.\n",
        "          *Why:* This builds reproducibility and early error detection.\n",
        "\n",
        "  # 4. Encourage experiment tracking\n",
        "  - trigger: on_notebook_open\n",
        "    condition: not notebook_contains(\"wandb\") and model_training_detected == true\n",
        "    action: >\n",
        "      insert_markdown_after_heading(\"Training\", \"\"\"\n",
        "      ### ðŸ“ˆ Learning Checkpoint: Experiment Tracking\n",
        "      Integrate Weights & Biases (wandb) or MLflow to log metrics and hyperparameters.\n",
        "      *Why:* Versioning experiments saves you from 'mystery improvements.'\n",
        "      \"\"\")\n",
        "      insert_code_after_heading(\"Training\", \"\"\"\n",
        "      # TODO: Add experiment tracking\n",
        "      import wandb\n",
        "      wandb.init(project=\"periospot_learning\")\n",
        "      wandb.log({\"loss\": loss, \"accuracy\": acc})\n",
        "      \"\"\")\n",
        "\n",
        "  # 5. Reinforce data ethics and bias reflection\n",
        "  - trigger: on_notebook_close\n",
        "    condition: dataset_detected == true and not notebook_contains(\"bias\") \n",
        "    action: >\n",
        "      append_to_notebook_end:\n",
        "        - markdown: |\n",
        "            ### âš–ï¸ Reflection Checkpoint: Bias and Fairness\n",
        "            **Task:** Examine potential biases in your dataset (class imbalance, demographic skew, etc.).\n",
        "            *Why:* Ethical evaluation prevents spurious conclusions and builds trust in models.\n",
        "        - code: |\n",
        "            # TODO: Evaluate bias\n",
        "            import pandas as pd\n",
        "            df['target'].value_counts(normalize=True).plot(kind='bar')\n",
        "\n",
        "  # 6. Missing RAG or data pipeline for text projects\n",
        "  - trigger: on_notebook_open\n",
        "    condition: notebook_contains(\"text\") or notebook_contains(\"nlp\") and not notebook_contains(\"rag\") and not notebook_contains(\"langchain\")\n",
        "    action: >\n",
        "      append_to_notebook_end:\n",
        "        - markdown: |\n",
        "            ### ðŸ§® Learning Checkpoint: Retrieval-Augmented Generation\n",
        "            Youâ€™re processing text but not retrieving context.\n",
        "            **Task:** integrate a simple RAG pipeline using LangChain or FAISS.\n",
        "            *Why:* RAG connects your models to domain knowledge, critical for Periospot AI.\n",
        "        - code: |\n",
        "            # TODO: Build minimal RAG prototype\n",
        "            from langchain.chains import RetrievalQA\n",
        "            from langchain.vectorstores import FAISS\n",
        "            # Build embeddings, create retriever, and connect to an LLM\n",
        "\n",
        "  # 7. Add README scaffolding if absent\n",
        "  - trigger: on_repo_open\n",
        "    condition: not repo_contains(\"README.md\")\n",
        "    action: >\n",
        "      create_file(\"README.md\", \"\"\"\n",
        "      # Project Title\n",
        "      ## Overview\n",
        "      Describe dataset, objective, metrics, and key learnings.\n",
        "      ## To-Dos\n",
        "      - [ ] Add explainability (SHAP/LIME)\n",
        "      - [ ] Integrate ResNet transfer learning\n",
        "      - [ ] Set up pytest and CI\n",
        "      - [ ] Reflect on bias\n",
        "      \"\"\")\n",
        "\n",
        "\n",
        "# .cursorrules\n",
        "# Francisco Barbosa â€“ AI Mentor Mode\n",
        "# Purpose: enforce discovery of higher-order ML/engineering practices in every Jupyter notebook.\n",
        "\n",
        "rules:\n",
        "  # 1. Detect CNN or image projects missing transfer learning\n",
        "  - trigger: on_notebook_open\n",
        "    condition: notebook_contains(\"torch\") and not notebook_contains(\"resnet\") and not notebook_contains(\"efficientnet\")\n",
        "    action: >\n",
        "      Insert_after_first_imports:\n",
        "        - markdown: |\n",
        "            ### ðŸ§© Learning Checkpoint: Transfer Learning with ResNet\n",
        "            Youâ€™re using CNNs but not leveraging pretrained backbones.\n",
        "            **Task:** integrate a pretrained ResNet (or EfficientNet) and compare its validation accuracy vs. your scratch model.\n",
        "            *Why:* Transfer learning dramatically reduces training time and improves generalization.\n",
        "        - code: |\n",
        "            # TODO: Implement transfer learning\n",
        "            from torchvision import models\n",
        "            resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "            for param in resnet.parameters():\n",
        "                param.requires_grad = False  # Freeze layers\n",
        "            # Replace final layer for your num_classes and fine-tune\n",
        "            # Compare performance with your baseline model.\n",
        "\n",
        "  # 2. Detect missing explainability\n",
        "  - trigger: on_notebook_save\n",
        "    condition: model_trained == true and not notebook_contains(\"shap\") and not notebook_contains(\"lime\")\n",
        "    action: >\n",
        "      append_to_notebook_end:\n",
        "        - markdown: |\n",
        "            ### ðŸ” Learning Checkpoint: Model Explainability\n",
        "            Youâ€™ve trained a modelâ€”now interpret it.\n",
        "            **Task:** apply SHAP or LIME to visualize which features drive predictions.\n",
        "            *Why:* Understanding model reasoning prevents overfitting and bias.\n",
        "        - code: |\n",
        "            # TODO: Add model explainability\n",
        "            import shap\n",
        "            explainer = shap.Explainer(model, X_valid)\n",
        "            shap_values = explainer(X_valid)\n",
        "            shap.summary_plot(shap_values, X_valid)\n",
        "\n",
        "  # 3. Enforce testing and CI mindset\n",
        "  - trigger: on_notebook_save\n",
        "    condition: not repo_contains(\"tests/\") and notebook_contains(\"model.fit\") or notebook_contains(\"train\")\n",
        "    action: >\n",
        "      create_file(\"tests/test_model.py\", \"\"\"\n",
        "      import pytest\n",
        "\n",
        "      def test_training_runs():\n",
        "          assert True, \"Add a small unit test for your training function\"\n",
        "      \"\"\")\n",
        "      insert_markdown_at_end: |\n",
        "          ### ðŸ§ª Learning Checkpoint: Testing & Automation\n",
        "          **Task:** create minimal Pytest tests for data loading and model training.\n",
        "          Later, set up a GitHub Action to run `pytest` on every push.\n",
        "          *Why:* This builds reproducibility and early error detection.\n",
        "\n",
        "  # 4. Encourage experiment tracking\n",
        "  - trigger: on_notebook_open\n",
        "    condition: not notebook_contains(\"wandb\") and model_training_detected == true\n",
        "    action: >\n",
        "      insert_markdown_after_heading(\"Training\", \"\"\"\n",
        "      ### ðŸ“ˆ Learning Checkpoint: Experiment Tracking\n",
        "      Integrate Weights & Biases (wandb) or MLflow to log metrics and hyperparameters.\n",
        "      *Why:* Versioning experiments saves you from 'mystery improvements.'\n",
        "      \"\"\")\n",
        "      insert_code_after_heading(\"Training\", \"\"\"\n",
        "      # TODO: Add experiment tracking\n",
        "      import wandb\n",
        "      wandb.init(project=\"periospot_learning\")\n",
        "      wandb.log({\"loss\": loss, \"accuracy\": acc})\n",
        "      \"\"\")\n",
        "\n",
        "  # 5. Reinforce data ethics and bias reflection\n",
        "  - trigger: on_notebook_close\n",
        "    condition: dataset_detected == true and not notebook_contains(\"bias\") \n",
        "    action: >\n",
        "      append_to_notebook_end:\n",
        "        - markdown: |\n",
        "            ### âš–ï¸ Reflection Checkpoint: Bias and Fairness\n",
        "            **Task:** Examine potential biases in your dataset (class imbalance, demographic skew, etc.).\n",
        "            *Why:* Ethical evaluation prevents spurious conclusions and builds trust in models.\n",
        "        - code: |\n",
        "            # TODO: Evaluate bias\n",
        "            import pandas as pd\n",
        "            df['target'].value_counts(normalize=True).plot(kind='bar')\n",
        "\n",
        "  # 6. Missing RAG or data pipeline for text projects\n",
        "  - trigger: on_notebook_open\n",
        "    condition: notebook_contains(\"text\") or notebook_contains(\"nlp\") and not notebook_contains(\"rag\") and not notebook_contains(\"langchain\")\n",
        "    action: >\n",
        "      append_to_notebook_end:\n",
        "        - markdown: |\n",
        "            ### ðŸ§® Learning Checkpoint: Retrieval-Augmented Generation\n",
        "            Youâ€™re processing text but not retrieving context.\n",
        "            **Task:** integrate a simple RAG pipeline using LangChain or FAISS.\n",
        "            *Why:* RAG connects your models to domain knowledge, critical for Periospot AI.\n",
        "        - code: |\n",
        "            # TODO: Build minimal RAG prototype\n",
        "            from langchain.chains import RetrievalQA\n",
        "            from langchain.vectorstores import FAISS\n",
        "            # Build embeddings, create retriever, and connect to an LLM\n",
        "\n",
        "  # 7. Add README scaffolding if absent\n",
        "  - trigger: on_repo_open\n",
        "    condition: not repo_contains(\"README.md\")\n",
        "    action: >\n",
        "      create_file(\"README.md\", \"\"\"\n",
        "      # Project Title\n",
        "      ## Overview\n",
        "      Describe dataset, objective, metrics, and key learnings.\n",
        "      ## To-Dos\n",
        "      - [ ] Add explainability (SHAP/LIME)\n",
        "      - [ ] Integrate ResNet transfer learning\n",
        "      - [ ] Set up pytest and CI\n",
        "      - [ ] Reflect on bias\n",
        "      \"\"\")\n",
        "ffffff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸš€ Next Steps\n",
        "\n",
        "Great work! You've built a training transform pipeline with augmentation.\n",
        "\n",
        "**Move to Notebook 02:** Train Dataset & DataLoader\n",
        "\n",
        "**Key Takeaway:** Transform order matters â€” Resize/Aug â†’ ToTensor â†’ Normalize!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
