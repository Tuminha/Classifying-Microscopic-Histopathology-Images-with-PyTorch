{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎨 Notebook 01: Train Transforms — Data Augmentation Pipeline\n",
    "\n",
    "**Purpose:** Build a robust augmentation pipeline for training data to improve model generalization.\n",
    "\n",
    "**What you'll learn:** How to compose transforms in the correct order, why augmentation matters, and how normalization stabilizes training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Concept Primer: Why Data Augmentation?\n",
    "\n",
    "### The Problem: Limited Data\n",
    "- Deep learning models need **thousands** of examples to generalize well\n",
    "- Medical imaging datasets are expensive to label (expert pathologists needed)\n",
    "- Small datasets → **overfitting** (model memorizes training examples)\n",
    "\n",
    "### The Solution: Data Augmentation\n",
    "- Create **synthetic variety** by applying realistic transformations\n",
    "- Horizontal flips, rotations, color jitter → model learns invariant features\n",
    "- **Only applied to training data** (val/test need consistency)\n",
    "\n",
    "### Transform Order Matters!\n",
    "```\n",
    "✅ CORRECT:\n",
    "Resize → Augmentation (Flip, Rotate, ColorJitter) → ToTensor → Normalize\n",
    "\n",
    "❌ WRONG:\n",
    "ToTensor → ColorJitter  (ColorJitter expects PIL images, not tensors!)\n",
    "Normalize → Resize  (Normalize expects [0,1] tensor values)\n",
    "```\n",
    "\n",
    "### Normalization Deep Dive\n",
    "- `ToTensor()` converts PIL image [0,255] → tensor [0,1]\n",
    "- `Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])` converts [0,1] → [-1,1]\n",
    "- **Formula:** `output = (input - mean) / std`\n",
    "- **Why?** Centered data → stable gradients → faster convergence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "1. ✅ Build `train_transform` with `transforms.Compose()`\n",
    "2. ✅ Apply augmentations in the correct order\n",
    "3. ✅ Understand which augmentations are realistic for histopathology\n",
    "4. ✅ Normalize images to stabilize training\n",
    "5. ✅ Verify transform output shape: `[3, 96, 96]`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Acceptance Criteria\n",
    "\n",
    "Your transform pipeline is correct when:\n",
    "\n",
    "- [ ] `train_transform` is a `transforms.Compose` object\n",
    "- [ ] Transforms are in order: Resize → RandomHorizontalFlip → RandomRotation → ColorJitter → ToTensor → Normalize\n",
    "- [ ] Applying transform to a sample image produces a tensor of shape `[3, 96, 96]`\n",
    "- [ ] Tensor values are in range `[-1, 1]` (after normalization)\n",
    "- [ ] You can explain why ColorJitter comes **before** ToTensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 💻 TODO 1: Import Required Libraries\n",
    "\n",
    "**What you need:**\n",
    "- `torchvision.transforms` for transform classes\n",
    "- `PIL.Image` to test loading a sample image\n",
    "\n",
    "**Expected behavior:** Imports run without errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1: Import transforms and Image\n",
    "# Hint: from torchvision import transforms\n",
    "# Hint: from PIL import Image\n",
    "\n",
    "# YOUR CODE HERE\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 💻 TODO 2: Build the Training Transform Pipeline\n",
    "\n",
    "**What you need to compose (in this order):**\n",
    "\n",
    "1. **`transforms.Resize((96, 96))`** — Ensure all images are 96×96\n",
    "2. **`transforms.RandomHorizontalFlip(p=0.5)`** — Flip left-right 50% of the time\n",
    "3. **`transforms.RandomRotation(degrees=15)`** — Rotate ±15° randomly\n",
    "4. **`transforms.ColorJitter(brightness=0.2, contrast=0.2)`** — Vary brightness/contrast\n",
    "5. **`transforms.ToTensor()`** — Convert PIL image → tensor [0,1]\n",
    "6. **`transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])`** — Normalize to [-1,1]\n",
    "\n",
    "**Expected output:** A `transforms.Compose` object stored in `train_transform`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ train_transform created:\n",
      "Compose(\n",
      "    Resize(size=(96, 96), interpolation=bilinear, max_size=None, antialias=True)\n",
      "    RandomHorizontalFlip(p=0.5)\n",
      "    ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=None, hue=None)\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# TODO 2: Create train_transform using transforms.Compose()\n",
    "# Hint: train_transform = transforms.Compose([...])\n",
    "# Hint: List the 6 transforms above in the correct order\n",
    "\n",
    "# YOUR CODE HERE\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((96, 96)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])  \n",
    "\n",
    "print(\"✅ train_transform created:\")\n",
    "print(train_transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 💻 TODO 3: Test the Transform on a Sample Image\n",
    "\n",
    "**What you need to do:**\n",
    "1. Load a sample image from `../data/pcam_images/` (pick any `.png` file)\n",
    "2. Apply `train_transform` to the image\n",
    "3. Print the shape of the resulting tensor\n",
    "\n",
    "**Expected output:**\n",
    "```\n",
    "Original image: PIL Image object\n",
    "Transformed tensor shape: torch.Size([3, 96, 96])\n",
    "Tensor value range: approximately [-1, 1]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 96, 96])\n",
      "First value of shape refers to the number of channels and in this case is: 3\n",
      "Second and third values refer to the height and width of the image and in this case are: 96 and 96\n",
      "The value range is: tensor(-0.8196) and tensor(1.)\n",
      "✅ Sample image path: ../data/pcam_images/348.png\n"
     ]
    }
   ],
   "source": [
    "# TODO 3: Test transform on a sample image\n",
    "# Hint: sample_img = Image.open('../data/pcam_images/SOME_FILE.png')\n",
    "# Hint: transformed = train_transform(sample_img)\n",
    "# Hint: print(transformed.shape)\n",
    "\n",
    "import os\n",
    "\n",
    "# Get a sample image path\n",
    "sample_images = os.listdir('../data/pcam_images/')\n",
    "sample_path = os.path.join('../data/pcam_images/', sample_images[0])\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Load the image\n",
    "with Image.open(sample_path) as sample_img:\n",
    "    transformed = train_transform(sample_img)\n",
    "# Apply train_transform\n",
    "    train_transform\n",
    "# Print the shape and value range\n",
    "    print(transformed.shape)\n",
    "    print(\"First value of shape refers to the number of channels and in this case is:\", transformed.shape[0])\n",
    "    print(\"Second and third values refer to the height and width of the image and in this case are:\", transformed.shape[1], \"and\", transformed.shape[2])\n",
    "    print(\"The value range is:\", transformed.min(), \"and\", transformed.max())\n",
    "print(f\"✅ Sample image path: {sample_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🤔 Reflection Prompts\n",
    "\n",
    "### Question 1: Realistic Augmentations for Histopathology\n",
    "Which of the following augmentations are **realistic** for H&E-stained pathology slides, and which might **distort clinically relevant information**?\n",
    "\n",
    "| Augmentation | Realistic? | Reasoning |\n",
    "|--------------|------------|-----------|\n",
    "| RandomHorizontalFlip | ✅ / ❌ | ? |\n",
    "| RandomVerticalFlip | ✅ / ❌ | ? |\n",
    "| RandomRotation(±15°) | ✅ / ❌ | ? |\n",
    "| RandomRotation(±180°) | ✅ / ❌ | ? |\n",
    "| ColorJitter(brightness=0.2) | ✅ / ❌ | ? |\n",
    "| ColorJitter(hue=0.5) | ✅ / ❌ | ? |\n",
    "| RandomGrayscale | ✅ / ❌ | ? |\n",
    "\n",
    "**Your analysis:**\n",
    "\n",
    "---\n",
    "\n",
    "### Question 2: Why Not Augment Validation/Test Data?\n",
    "Explain in your own words:\n",
    "- Why do we apply augmentation to training data?\n",
    "- Why would augmentation **break** validation/test evaluation?\n",
    "\n",
    "**Your explanation:**\n",
    "> We augment training data to teach the model to handle natural variations (orientation, lighting, color shifts), preventing it from memorizing specific training examples. This helps generalization.\n",
    ">\n",
    "> We DON'T augment validation/test because we need **consistent, reproducible evaluation**. Each validation/test image should be processed the same way every time, so we can accurately measure model performance and track improvements.\n",
    ">\n",
    "> If we augmented validation data randomly each time, we'd get different results each run—making it impossible to know if the model improved or if we just got \"lucky\" augmented images that round!\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3: Normalization Intuition\n",
    "Given:\n",
    "- `ToTensor()` converts RGB [0,255] → [0,1]\n",
    "- `Normalize(mean=[0.5]*3, std=[0.5]*3)` converts [0,1] → [-1,1]\n",
    "\n",
    "Calculate:\n",
    "- If a pixel value is `0.8` after `ToTensor()`, what is it after `Normalize`?\n",
    "- **Formula:** `(input - mean) / std`\n",
    "\n",
    "**Your calculation:**\n",
    "> Using the formula: `output = (input - mean) / std`\n",
    "> \n",
    "> `output = (0.8 - 0.5) / 0.5`\n",
    "> `output = 0.3 / 0.5`\n",
    "> `output = 0.6`\n",
    ">\n",
    "> So a pixel value of `0.8` becomes `0.6` after normalization!\n",
    ">\n",
    "> **Visual meaning:** The pixel stays bright (80% → 60% of max), but now it's centered around 0 instead of 0.5. This helps stabilize training by keeping gradients centered.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Next Steps\n",
    "\n",
    "Great work! You've built a training transform pipeline with augmentation.\n",
    "\n",
    "**Move to Notebook 02:** Train Dataset & DataLoader\n",
    "\n",
    "**Key Takeaway:** Transform order matters — Resize/Aug → ToTensor → Normalize!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Codecademy ML",
   "language": "python",
   "name": "codeacademy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
