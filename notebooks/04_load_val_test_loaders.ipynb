{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 📊 Notebook 04: Val/Test DataLoaders\n",
        "\n",
        "**Purpose:** Create DataLoaders for validation and test sets with `shuffle=False` and larger batch sizes.\n",
        "\n",
        "**What you'll learn:** How evaluation data loading differs from training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎯 Concept Primer: Evaluation DataLoaders\n",
        "\n",
        "### Key Differences: Training vs Evaluation\n",
        "\n",
        "| Parameter | Training | Validation | Test |\n",
        "|-----------|----------|------------|------|\n",
        "| `shuffle` | ✅ True | ❌ False | ❌ False |\n",
        "| `batch_size` | Small (8-16) | Large (32-64) | Large (32-64) |\n",
        "| `drop_last` | Optional | ❌ False | ❌ False |\n",
        "\n",
        "### Why shuffle=False for Evaluation?\n",
        "- Consistent, repeatable metrics\n",
        "- Easier debugging (same order every time)\n",
        "- No benefit from shuffling (no training happens)\n",
        "\n",
        "### Why Larger Batch Sizes for Evaluation?\n",
        "- **No backpropagation** → less memory needed\n",
        "- **Faster inference** → fewer iterations\n",
        "- Batch size of 32 or 64 is common\n",
        "\n",
        "### drop_last Consideration\n",
        "- `drop_last=True`: Drops the last incomplete batch\n",
        "- **Training:** Sometimes used to keep batch sizes consistent\n",
        "- **Evaluation:** Always `False` (we want to evaluate ALL samples!)\n",
        "\n",
        "**Example:** 100 samples, batch_size=32\n",
        "- Batches: [32, 32, 32, 4]\n",
        "- If `drop_last=True`, we'd skip 4 samples → biased metrics!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📚 Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "\n",
        "1. ✅ Create `val_dataset` and `test_dataset` with `val_test_transform`\n",
        "2. ✅ Create `val_dataloader` and `test_dataloader` with `shuffle=False`\n",
        "3. ✅ Use `batch_size=32` for faster evaluation\n",
        "4. ✅ Verify shapes: `images=[32,3,96,96]`, `labels=[32]` (or smaller for last batch)\n",
        "5. ✅ Understand why evaluation doesn't shuffle\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ✅ Acceptance Criteria\n",
        "\n",
        "Your val/test data loaders are correct when:\n",
        "\n",
        "- [ ] `val_dataset` and `test_dataset` use `val_test_transform`\n",
        "- [ ] `val_dataloader` and `test_dataloader` have `shuffle=False`\n",
        "- [ ] Both use `batch_size=32`\n",
        "- [ ] Iterating one batch produces `images.shape = [32, 3, 96, 96]` (or less for last batch)\n",
        "- [ ] Running the iteration twice yields batches in the **same order**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 💻 TODO 1: Import Libraries & Rebuild val_test_transform\n",
        "\n",
        "**What you need:**\n",
        "- PyTorch, DataLoader, transforms\n",
        "- `PCamDataset`\n",
        "- Rebuild `val_test_transform` from Notebook 03\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Imports successful and val_test_transform ready\n"
          ]
        }
      ],
      "source": [
        "# TODO 1: Import libraries and rebuild val_test_transform\n",
        "# Hint: import torch\n",
        "# Hint: from torch.utils.data import DataLoader\n",
        "# Hint: from torchvision import transforms\n",
        "# Hint: from src.datasets.pcam_dataset import PCamDataset\n",
        "\n",
        "# YOUR CODE HERE\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "from src.datasets.pcam_dataset import PCamDataset\n",
        "\n",
        "# Rebuild val_test_transform (copy from Notebook 03)\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize((96, 96)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "print(\"✅ Imports successful and val_test_transform ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 💻 TODO 2: Create Validation & Test Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Validation dataset: 200 samples\n",
            "✅ Test dataset: 200 samples\n"
          ]
        }
      ],
      "source": [
        "# TODO 2: Create val_dataset and test_dataset\n",
        "# Hint: val_dataset = PCamDataset(csv_file='../data/validation_labels.csv', transform=val_test_transform)\n",
        "# Hint: test_dataset = PCamDataset(csv_file='../data/test_labels.csv', transform=val_test_transform)\n",
        "\n",
        "# YOUR CODE HERE\n",
        "val_dataset = PCamDataset(csv_file='../data/validation_labels.csv', transform=val_test_transform)  # Replace this line\n",
        "test_dataset = PCamDataset(csv_file='../data/test_labels.csv', transform=val_test_transform)\n",
        "\n",
        "print(f\"✅ Validation dataset: {len(val_dataset)} samples\")\n",
        "print(f\"✅ Test dataset: {len(test_dataset)} samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 💻 TODO 3: Create Validation & Test DataLoaders (shuffle=False!)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Validation DataLoader: 7 batches\n",
            "✅ Test DataLoader: 7 batches\n"
          ]
        }
      ],
      "source": [
        "# TODO 3: Create val_dataloader and test_dataloader\n",
        "# Hint: DataLoader(dataset, batch_size=32, shuffle=False, num_workers=0)\n",
        "\n",
        "# YOUR CODE HERE\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)  # Replace this line\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
        "\n",
        "print(f\"✅ Validation DataLoader: {len(val_dataloader)} batches\")\n",
        "print(f\"✅ Test DataLoader: {len(test_dataloader)} batches\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 💻 TODO 4: Test the Val DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Validation batch loaded\n",
            "   Images shape: torch.Size([32, 3, 96, 96])\n",
            "   Labels shape: torch.Size([32])\n",
            "   Batch size: 32\n",
            "✅ Test batch loaded\n",
            "   Images shape: torch.Size([32, 3, 96, 96])\n",
            "   Labels shape: torch.Size([32])\n",
            "   Batch size: 32\n"
          ]
        }
      ],
      "source": [
        "# TODO 4: Iterate one batch from val_dataloader\n",
        "# Hint: for images, labels in val_dataloader: ...\n",
        "\n",
        "# YOUR CODE HERE\n",
        "for images, labels in val_dataloader:\n",
        "    print(f\"✅ Validation batch loaded\")\n",
        "    print(f\"   Images shape: {images.shape}\")\n",
        "    print(f\"   Labels shape: {labels.shape}\")\n",
        "    print(f\"   Batch size: {len(labels)}\")\n",
        "    break\n",
        "\n",
        "for images, labels in test_dataloader:\n",
        "    print(f\"✅ Test batch loaded\")\n",
        "    print(f\"   Images shape: {images.shape}\")\n",
        "    print(f\"   Labels shape: {labels.shape}\")\n",
        "    print(f\"   Batch size: {len(labels)}\")\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 🤔 Reflection Prompts\n",
        "\n",
        "### Question 1: Why Larger Batch Sizes for Evaluation?\n",
        "Training uses `batch_size=8`, but evaluation uses `batch_size=32`.\n",
        "\n",
        "**Question:** What allows us to use larger batches during evaluation?\n",
        "\n",
        "**Your answer:**\n",
        "> During evaluation, we don't need to compute gradients or backpropagation - it's just forward pass for comparison. We don't need the computational power for training, so we can use larger batch sizes like 32 instead of 8, which makes evaluation much faster. We don't cut samples in validation/test because we need all samples for accurate evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "### Question 2: Consistency Check\n",
        "Run this code twice:\n",
        "\n",
        "```python\n",
        "first_batch_labels_run1 = next(iter(val_dataloader))[1]\n",
        "first_batch_labels_run2 = next(iter(val_dataloader))[1]\n",
        "```\n",
        "\n",
        "**Questions:**\n",
        "- Will `first_batch_labels_run1` equal `first_batch_labels_run2`?\n",
        "- Why or why not?\n",
        "- What if `val_dataloader` had `shuffle=True`?\n",
        "\n",
        "**Your analysis:**\n",
        "> Yes, they should be equal because we use `shuffle=False` for validation. The batches will be the same each time, ensuring consistent evaluation. The last batch might be smaller if the total samples don't divide evenly by batch size (e.g., 70 samples with batch_size=32 gives batches of 32, 32, and 6), but this is expected and we don't cut samples in validation/test.\n",
        ">\n",
        "> If `val_dataloader` had `shuffle=True`, it would create randomness and we would never have correct validation results. Each run would give different results, making it impossible to track model performance consistently.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🚀 Next Steps\n",
        "\n",
        "Excellent! You now have all three data loaders ready.\n",
        "\n",
        "**Move to Notebook 05:** Simple CNN Architecture\n",
        "\n",
        "**Key Takeaway:** Train=shuffle, Val/Test=no shuffle + larger batches!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Codecademy ML",
      "language": "python",
      "name": "codeacademy"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
