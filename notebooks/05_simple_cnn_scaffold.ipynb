{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üèóÔ∏è Notebook 05: Simple CNN Architecture\n",
        "\n",
        "**Purpose:** Design and implement a convolutional neural network from scratch for binary classification.\n",
        "\n",
        "**What you'll learn:** How Conv2D, ReLU, MaxPool, and fully connected layers work together to classify images.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Concept Primer: CNN Building Blocks\n",
        "\n",
        "### Convolutional Layers (Conv2D)\n",
        "- **Purpose:** Detect local patterns (edges, textures, shapes)\n",
        "- **Parameters:**\n",
        "  - `in_channels`: Number of input channels (3 for RGB)\n",
        "  - `out_channels`: Number of filters/feature maps (32, 64, 128)\n",
        "  - `kernel_size`: Size of convolution window (3√ó3 common)\n",
        "  - `padding`: Adds zeros around image to preserve spatial size\n",
        "\n",
        "**Example:** Conv2D(3, 32, kernel_size=3, padding=1)\n",
        "- Input: [Batch, 3, 96, 96]\n",
        "- Output: [Batch, 32, 96, 96] (with padding=1)\n",
        "\n",
        "### ReLU Activation\n",
        "- **Formula:** `ReLU(x) = max(0, x)`\n",
        "- **Purpose:** Introduces non-linearity (allows learning complex patterns)\n",
        "- Applied after each convolution\n",
        "\n",
        "### MaxPool2D\n",
        "- **Purpose:** Reduces spatial dimensions, keeps strongest features\n",
        "- **Typical:** `MaxPool2D(2)` ‚Üí Halves width and height\n",
        "\n",
        "**Example:** MaxPool2D(2) on [Batch, 32, 96, 96]\n",
        "- Output: [Batch, 32, 48, 48]\n",
        "\n",
        "### Flatten\n",
        "- Converts 2D feature maps ‚Üí 1D vector for fully connected layers\n",
        "- **Example:** [Batch, 128, 12, 12] ‚Üí [Batch, 18432]\n",
        "\n",
        "### Fully Connected (Linear) Layers\n",
        "- **Purpose:** Combine global features for classification\n",
        "- **Final layer:** 1 output neuron for binary classification\n",
        "\n",
        "### Sigmoid Activation\n",
        "- **Formula:** `œÉ(x) = 1 / (1 + e^(-x))`\n",
        "- **Output:** Probability in [0, 1]\n",
        "- Applied to final layer for binary classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìê Architecture Specification\n",
        "\n",
        "### SimpleCNN: 3 Conv Blocks + 2 FC Layers\n",
        "\n",
        "```\n",
        "Input: [Batch, 3, 96, 96]\n",
        "    ‚Üì\n",
        "BLOCK 1:\n",
        "  Conv2D(in=3, out=32, kernel=3, padding=1)\n",
        "  ReLU\n",
        "  MaxPool2D(2)\n",
        "    ‚Üí [Batch, 32, 48, 48]\n",
        "    ‚Üì\n",
        "BLOCK 2:\n",
        "  Conv2D(in=32, out=64, kernel=3, padding=1)\n",
        "  ReLU\n",
        "  MaxPool2D(2)\n",
        "    ‚Üí [Batch, 64, 24, 24]\n",
        "    ‚Üì\n",
        "BLOCK 3:\n",
        "  Conv2D(in=64, out=128, kernel=3, padding=1)\n",
        "  ReLU\n",
        "  MaxPool2D(2)\n",
        "    ‚Üí [Batch, 128, 12, 12]\n",
        "    ‚Üì\n",
        "Flatten: [Batch, 128√ó12√ó12] = [Batch, 18432]\n",
        "    ‚Üì\n",
        "FC1: Linear(18432, 256)\n",
        "ReLU\n",
        "    ‚Üí [Batch, 256]\n",
        "    ‚Üì\n",
        "FC2: Linear(256, 1)\n",
        "Sigmoid\n",
        "    ‚Üí [Batch, 1]\n",
        "    ‚Üì\n",
        "Squeeze(dim=1)  ‚Üí [Batch]\n",
        "```\n",
        "\n",
        "### Spatial Dimension Tracking\n",
        "- 96 ‚Üí 48 ‚Üí 24 ‚Üí 12 (three halvings via MaxPool)\n",
        "- Final flatten size: 128 √ó 12 √ó 12 = **18,432**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìö Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "\n",
        "1. ‚úÖ Define a `SimpleCNN` class inheriting from `nn.Module`\n",
        "2. ‚úÖ Implement `__init__()` to define layers\n",
        "3. ‚úÖ Implement `forward()` to define the computation flow\n",
        "4. ‚úÖ Instantiate `cnn_model` and print its architecture\n",
        "5. ‚úÖ Test forward pass on a fake batch to verify output shape: `[Batch]`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Acceptance Criteria\n",
        "\n",
        "Your CNN is correct when:\n",
        "\n",
        "- [ ] `SimpleCNN` inherits from `nn.Module`\n",
        "- [ ] `__init__()` defines 3 Conv2D layers, 2 Linear layers\n",
        "- [ ] `forward()` applies Conv‚ÜíReLU‚ÜíMaxPool three times, then Flatten‚ÜíFC‚ÜíReLU‚ÜíFC‚ÜíSigmoid‚ÜíSqueeze\n",
        "- [ ] Forward pass on input `[8,3,96,96]` produces output shape `[8]`\n",
        "- [ ] Output values are in range [0, 1] (probabilities)\n",
        "- [ ] `print(cnn_model)` displays all layers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üíª TODO 1: Import PyTorch Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 1: Import PyTorch\n",
        "# Hint: import torch\n",
        "# Hint: import torch.nn as nn\n",
        "# Hint: import torch.nn.functional as F\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "print(\"‚úÖ PyTorch imported\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üíª TODO 2: Define SimpleCNN Class ‚Äî __init__ Method\n",
        "\n",
        "**What to define in `__init__`:**\n",
        "\n",
        "```python\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        \n",
        "        # TODO: Define convolutional layers\n",
        "        self.conv1 = nn.Conv2D(???, ???, kernel_size=???, padding=???)\n",
        "        self.conv2 = ???\n",
        "        self.conv3 = ???\n",
        "        \n",
        "        # TODO: Define fully connected layers\n",
        "        self.fc1 = nn.Linear(???, ???)\n",
        "        self.fc2 = nn.Linear(???, ???)\n",
        "```\n",
        "\n",
        "**Hints:**\n",
        "- `conv1`: 3 ‚Üí 32 channels\n",
        "- `conv2`: 32 ‚Üí 64 channels\n",
        "- `conv3`: 64 ‚Üí 128 channels\n",
        "- All convs: kernel_size=3, padding=1\n",
        "- `fc1`: 18432 ‚Üí 256\n",
        "- `fc2`: 256 ‚Üí 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 2: Define SimpleCNN class with __init__ method\n",
        "# Hint: class SimpleCNN(nn.Module):\n",
        "# Hint:     def __init__(self):\n",
        "# Hint:         super(SimpleCNN, self).__init__()\n",
        "# Hint:         # Define layers here\n",
        "\n",
        "# YOUR CODE HERE\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        \n",
        "        # TODO: Define conv1, conv2, conv3\n",
        "        # TODO: Define fc1, fc2\n",
        "        pass  # Remove this line when you add code\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # We'll implement this in the next TODO\n",
        "        pass\n",
        "\n",
        "print(\"‚úÖ SimpleCNN class defined (skeleton)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üíª TODO 3: Implement forward() Method\n",
        "\n",
        "**What the forward method should do:**\n",
        "\n",
        "```python\n",
        "def forward(self, x):\n",
        "    # Block 1: Conv1 ‚Üí ReLU ‚Üí MaxPool\n",
        "    # TODO: x = self.conv1(x)\n",
        "    # TODO: x = F.relu(x)\n",
        "    # TODO: x = F.max_pool2d(x, 2)  # or nn.MaxPool2d(2)\n",
        "    \n",
        "    # Block 2: Conv2 ‚Üí ReLU ‚Üí MaxPool\n",
        "    # TODO: ...\n",
        "    \n",
        "    # Block 3: Conv3 ‚Üí ReLU ‚Üí MaxPool\n",
        "    # TODO: ...\n",
        "    \n",
        "    # Flatten: [Batch, 128, 12, 12] ‚Üí [Batch, 18432]\n",
        "    # TODO: x = x.view(x.size(0), -1)  # or torch.flatten(x, 1)\n",
        "    \n",
        "    # FC1 ‚Üí ReLU\n",
        "    # TODO: x = self.fc1(x)\n",
        "    # TODO: x = F.relu(x)\n",
        "    \n",
        "    # FC2 ‚Üí Sigmoid ‚Üí Squeeze\n",
        "    # TODO: x = self.fc2(x)\n",
        "    # TODO: x = torch.sigmoid(x)\n",
        "    # TODO: x = x.squeeze(1)  # [Batch, 1] ‚Üí [Batch]\n",
        "    \n",
        "    return x\n",
        "```\n",
        "\n",
        "**Expected shapes at each step:**\n",
        "- After Conv1+Pool: [B, 32, 48, 48]\n",
        "- After Conv2+Pool: [B, 64, 24, 24]\n",
        "- After Conv3+Pool: [B, 128, 12, 12]\n",
        "- After Flatten: [B, 18432]\n",
        "- After FC1: [B, 256]\n",
        "- After FC2: [B, 1]\n",
        "- After Squeeze: [B]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 3: Implement the forward() method\n",
        "# Re-define the class with complete forward() implementation\n",
        "\n",
        "# YOUR CODE HERE\n",
        "# Copy the class from TODO 2 and complete the forward() method\n",
        "\n",
        "print(\"‚úÖ SimpleCNN forward() method implemented\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üíª TODO 4: Instantiate the Model & Print Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 4: Instantiate cnn_model and print it\n",
        "# Hint: cnn_model = SimpleCNN()\n",
        "# Hint: print(cnn_model)\n",
        "\n",
        "# YOUR CODE HERE\n",
        "cnn_model = None  # Replace this line\n",
        "\n",
        "print(\"‚úÖ Model instantiated:\")\n",
        "# Print the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üíª TODO 5: Test Forward Pass with Fake Batch\n",
        "\n",
        "**What you need to do:**\n",
        "1. Create a fake batch: `fake_batch = torch.randn(8, 3, 96, 96)`\n",
        "2. Run forward pass: `outputs = cnn_model(fake_batch)`\n",
        "3. Print output shape and value range\n",
        "\n",
        "**Expected output:**\n",
        "```\n",
        "‚úÖ Forward pass successful\n",
        "   Input shape: torch.Size([8, 3, 96, 96])\n",
        "   Output shape: torch.Size([8])\n",
        "   Output range: [min_val, max_val]  (should be between 0 and 1)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 5: Test forward pass with a fake batch\n",
        "# Hint: fake_batch = torch.randn(8, 3, 96, 96)\n",
        "# Hint: outputs = cnn_model(fake_batch)\n",
        "# Hint: print(outputs.shape)\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "print(\"‚úÖ Forward pass successful\")\n",
        "# Print input shape, output shape, min/max output values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ü§î Reflection Prompts\n",
        "\n",
        "### Question 1: Why Sigmoid at the End?\n",
        "We use Sigmoid for the final activation, producing outputs in [0, 1].\n",
        "\n",
        "**Alternative:** Use no final activation and `BCEWithLogitsLoss` instead of `BCELoss`.\n",
        "\n",
        "**Question:** What are the trade-offs between:\n",
        "- Sigmoid + BCELoss (what we use)\n",
        "- No Sigmoid + BCEWithLogitsLoss\n",
        "\n",
        "**Your analysis:**\n",
        "\n",
        "---\n",
        "\n",
        "### Question 2: Flattening Calculation\n",
        "After three MaxPool operations, we have shape [Batch, 128, 12, 12].\n",
        "\n",
        "**Calculate:**\n",
        "- How many parameters does `fc1 = nn.Linear(18432, 256)` have?\n",
        "- Formula: `num_params = (input_size √ó output_size) + output_size` (weights + biases)\n",
        "\n",
        "**Your calculation:**\n",
        "\n",
        "---\n",
        "\n",
        "### Question 3: Receptive Field\n",
        "Each conv layer with kernel_size=3 expands the receptive field.\n",
        "\n",
        "**Question:** After 3 conv layers, how much of the original input does a single neuron \"see\"?\n",
        "(Hint: This is a theoretical question about the receptive field growing with each layer.)\n",
        "\n",
        "**Your intuition:**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Next Steps\n",
        "\n",
        "Amazing! You've built a CNN from scratch.\n",
        "\n",
        "**Move to Notebook 06:** Device, Loss, and Optimizer Setup\n",
        "\n",
        "**Key Takeaway:** Conv‚ÜíReLU‚ÜíPool extracts features; FC layers classify!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
