{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ“Š Notebook 08: Test Inference & Classification Metrics\n",
        "\n",
        "**Purpose:** Run inference on the test set and generate a comprehensive classification report.\n",
        "\n",
        "**What you'll learn:** How to convert probabilities to class labels, collect predictions, and evaluate with precision/recall/F1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŽ¯ Concept Primer: From Probabilities to Metrics\n",
        "\n",
        "### Inference Pipeline\n",
        "```python\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_dataloader:\n",
        "        outputs = model(images)  # Probabilities in [0,1]\n",
        "        predictions = torch.round(outputs)  # 0 or 1\n",
        "```\n",
        "\n",
        "### Probability â†’ Class Label\n",
        "- **Threshold:** 0.5 (default)\n",
        "- `output >= 0.5` â†’ Class 1 (Tumor)\n",
        "- `output < 0.5` â†’ Class 0 (Normal)\n",
        "- **torch.round():** Applies threshold automatically\n",
        "\n",
        "### Classification Metrics\n",
        "\n",
        "**Precision:** Of predictions labeled Tumor, how many were actually Tumor?\n",
        "- Formula: `TP / (TP + FP)`\n",
        "\n",
        "**Recall:** Of actual Tumor samples, how many did we catch?\n",
        "- Formula: `TP / (TP + FN)`\n",
        "\n",
        "**F1-Score:** Harmonic mean of precision and recall\n",
        "- Formula: `2 Ã— (Precision Ã— Recall) / (Precision + Recall)`\n",
        "\n",
        "### Clinical Context\n",
        "- **False Negative (FN):** Miss a tumor â†’ Delayed treatment (VERY BAD)\n",
        "- **False Positive (FP):** Flag normal as tumor â†’ Unnecessary biopsy (BAD)\n",
        "- For medical AI: **High recall often prioritized** (catch all tumors)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“š Learning Objectives\n",
        "\n",
        "1. âœ… Run inference on test_dataloader (eval mode + no_grad)\n",
        "2. âœ… Collect probabilities and convert to class labels\n",
        "3. âœ… Gather true labels from test set\n",
        "4. âœ… Generate classification_report with class names\n",
        "5. âœ… Interpret precision, recall, F1 for Normal vs Tumor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âœ… Acceptance Criteria\n",
        "\n",
        "- [ ] Test inference completes without errors\n",
        "- [ ] `test_pred_labels` and `test_true_labels` are NumPy arrays\n",
        "- [ ] Both have same length (number of test samples)\n",
        "- [ ] `classification_report` prints with class names ['Normal', 'Tumor']\n",
        "- [ ] Report shows precision, recall, F1 for each class\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ’» TODO 1: Import & Rebuild Model + Test Loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 1: Import libraries and rebuild trained model + test_dataloader\n",
        "# Hint: from sklearn.metrics import classification_report\n",
        "# Hint: import numpy as np\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "print(\"âœ… Model and test loader ready for inference\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ’» TODO 2: Run Inference & Collect Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 2: Run inference on test set\n",
        "# Hint: test_pred_probs = []\n",
        "# Hint: test_true_labels = []\n",
        "# Hint: cnn_model.eval()\n",
        "# Hint: with torch.no_grad():\n",
        "# Hint:     for images, labels in test_dataloader:\n",
        "# Hint:         outputs = cnn_model(images.to(device))\n",
        "# Hint:         test_pred_probs.extend(outputs.cpu().numpy())\n",
        "# Hint:         test_true_labels.extend(labels.numpy())\n",
        "\n",
        "# YOUR CODE HERE\n",
        "test_pred_probs = []\n",
        "test_pred_labels = []\n",
        "test_true_labels = []\n",
        "\n",
        "# After loop: convert probs to labels using threshold 0.5\n",
        "# test_pred_labels = np.round(np.array(test_pred_probs))\n",
        "\n",
        "print(f\"âœ… Collected {len(test_pred_labels)} predictions\")\n",
        "print(f\"   Predicted classes: {np.unique(test_pred_labels)}\")\n",
        "print(f\"   True classes: {np.unique(test_true_labels)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ’» TODO 3: Generate Classification Report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 3: Print classification report\n",
        "# Hint: pcam_classes = ['Normal', 'Tumor']\n",
        "# Hint: print(classification_report(test_true_labels, test_pred_labels, target_names=pcam_classes))\n",
        "\n",
        "# YOUR CODE HERE\n",
        "pcam_classes = ['Normal', 'Tumor']\n",
        "\n",
        "print(\"\\\\nðŸ“Š Test Set Classification Report:\")\n",
        "print(\"=\"*60)\n",
        "# Print classification report here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ¤” Reflection Prompts\n",
        "\n",
        "### Question 1: Clinical Consequences\n",
        "Imagine your model achieves:\n",
        "- **Normal:** Precision=0.92, Recall=0.85\n",
        "- **Tumor:** Precision=0.88, Recall=0.94\n",
        "\n",
        "**Questions:**\n",
        "- Which class has more False Negatives?\n",
        "- Which error is worse clinically?\n",
        "- Would you adjust the threshold (0.5) for medical use?\n",
        "\n",
        "**Your analysis:**\n",
        "\n",
        "---\n",
        "\n",
        "### Question 2: Threshold Tuning\n",
        "Default threshold is 0.5. What if you changed it?\n",
        "\n",
        "| Threshold | Effect on Predictions |\n",
        "|-----------|----------------------|\n",
        "| 0.3 (lower) | ? |\n",
        "| 0.5 (current) | ? |\n",
        "| 0.7 (higher) | ? |\n",
        "\n",
        "**Your predictions:**\n",
        "\n",
        "---\n",
        "\n",
        "### Question 3: Perfect Metrics?\n",
        "If a model achieves 100% accuracy, precision, and recall on test data:\n",
        "\n",
        "**Question:** Is this always good? What might it indicate?\n",
        "\n",
        "**Your answer:**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸš€ Next Steps\n",
        "\n",
        "**Project Complete!** Move to Notebook 99 to document your learning journey.\n",
        "\n",
        "**Optional Next Steps:**\n",
        "- Experiment with deeper architectures (ResNet)\n",
        "- Try different learning rates\n",
        "- Implement early stopping\n",
        "- Visualize misclassified samples\n",
        "- Explore Grad-CAM for interpretability\n",
        "\n",
        "**Key Takeaway:** Metrics must be interpreted in clinical context!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
